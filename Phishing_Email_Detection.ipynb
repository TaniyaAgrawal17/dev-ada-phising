{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaniyaAgrawal17/dev-ada-phising/blob/main/Phishing_Email_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "5Dlp4NeFyVRF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtgPwIf323Qb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59e90b2f-5e03-4133-8345-a4385580f206"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import ssl\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download dataset"
      ],
      "metadata": {
        "id": "sHZV2Vnnyagp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjJxsWPPuvmb",
        "outputId": "7e41a1bf-000d-42f5-a86e-f75266f9ec55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.10)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/naserabdullahalam/phishing-email-dataset?select=phishing_email.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdCtEsmKu52L",
        "outputId": "c3cf522c-ef0c-49bb-da48-efe2394c6442"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: carolineef\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/naserabdullahalam/phishing-email-dataset\n",
            "Downloading phishing-email-dataset.zip to ./phishing-email-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 77.1M/77.1M [00:00<00:00, 125MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.listdir(\"/content\"))\n",
        "print(os.listdir(\"/content/phishing-email-dataset\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ9GyK3WwUON",
        "outputId": "cf35c77a-6784-41e2-b6aa-919db6e777e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'phishing-email-dataset', 'sample_data']\n",
            "['Ling.csv', 'CEAS_08.csv', 'phishing_email.csv', 'SpamAssasin.csv', 'Enron.csv', 'Nigerian_Fraud.csv', 'Nazario.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/phishing-email-dataset/phishing_email.csv\")\n",
        "df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "lk3W3YGlyPs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning data"
      ],
      "metadata": {
        "id": "oIyVHA2fye9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, unwanted_terms):\n",
        "    text = text.lower()\n",
        "    # Removes numbers\n",
        "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
        "    # Removes HTML tags\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    # Removes dates\n",
        "    text = re.sub(r'\\b(?:mon|tue|wed|thu|fri|sat|sun)\\b', '', text, flags=re.IGNORECASE)\n",
        "    text = re.sub(r'\\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\b', '', text, flags=re.IGNORECASE)\n",
        "    # Removes terms that frequently show up but have no meaning\n",
        "    for term in unwanted_terms:\n",
        "        text = re.sub(rf'\\b{re.escape(term)}\\b', '', text, flags=re.IGNORECASE)\n",
        "    # Removes punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Removes extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Removes words that are underscores\n",
        "    text = re.sub(r'\\b_+\\b', '', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "unwanted_terms = ['enron', 'hpl', 'nom', 'forwarded', '2008', '10', 'hplno', 'xls']\n",
        "df['text_combined'] = df['text_combined'].apply(lambda x: preprocess_text(x, unwanted_terms))\n",
        "\n",
        "def get_top_n_words(text, n=10):\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "    word_count = vectorizer.fit_transform(text)\n",
        "    word_freq = dict(zip(vectorizer.get_feature_names_out(), word_count.sum(axis=0).A1))\n",
        "    sorted_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_words[:n]\n",
        "\n",
        "phishing_text = df[df['label'] == 1]['text_combined']\n",
        "non_phishing_text = df[df['label'] == 0]['text_combined']\n",
        "\n",
        "# Top 10 Words\n",
        "phishing_top_words = get_top_n_words(phishing_text, n=10)\n",
        "non_phishing_top_words = get_top_n_words(non_phishing_text, n=10)\n",
        "\n",
        "# Convert to dataframe\n",
        "phishing_df = pd.DataFrame(phishing_top_words, columns=['Word', 'Frequency'])\n",
        "non_phishing_df = pd.DataFrame(non_phishing_top_words, columns=['Word', 'Frequency'])\n",
        "\n",
        "# print(\"Top 10 Words in Phishing Emails:\")\n",
        "# for word, freq in phishing_top_words:\n",
        "#     print(f\"{word}: {freq}\")\n",
        "\n",
        "# print(\"\\nTop 10 Words in Non-Phishing Emails:\")\n",
        "# for word, freq in non_phishing_top_words:\n",
        "#     print(f\"{word}: {freq}\")\n",
        "\n",
        "# # Word Cloud for Phishing Emails\n",
        "# wordcloud_phishing = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(dict(phishing_top_words))\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.imshow(wordcloud_phishing, interpolation=\"bilinear\")\n",
        "# plt.axis(\"off\")\n",
        "# plt.title(\"Word Cloud - Phishing Emails\")\n",
        "# plt.show()\n",
        "\n",
        "# # Word Cloud for Non-Phishing Emails\n",
        "# wordcloud_non_phishing = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(dict(non_phishing_top_words))\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.imshow(wordcloud_non_phishing, interpolation=\"bilinear\")\n",
        "# plt.axis(\"off\")\n",
        "# plt.title(\"Word Cloud - Non-Phishing Emails\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "Hk3Dm-eayiaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "5_Rh9yg8zYPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TF_IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "# X is an input matrix where each row represents an email and each column represents a word's importance\n",
        "X = vectorizer.fit_transform(df['text_combined'])\n",
        "y = df['label']\n",
        "\n",
        "# print(df['text_combined'].head(10))\n",
        "# print(vectorizer.get_feature_names_out()[:50])\n",
        "\n",
        "# Create data frame of tfidf matrix\n",
        "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n"
      ],
      "metadata": {
        "id": "VkYWd5hCzbDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "2KKNdkcJ-iz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into training (80%) and testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train the model\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = svm_model.score(X_test, y_test)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "Dmo_MMIG-xeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38e02c45-d1b8-44bb-f60a-36a037f79b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9809941520467836\n"
          ]
        }
      ]
    }
  ]
}